import os
import json
from time import time

from openai import OpenAI

from search import elastic_search_hybrid
from cost_calculator import calculate_openai_cost
from evaluate import evaluate_relevance
from promt_storage import PROMT_TEMPLTE, PROMT_TEMPLTE_CONVERSATION, ENTRY_TEMPLATE

client = OpenAI()

def build_prompt(query, search_results):
    context = ""

    for doc in search_results:
        context = context + ENTRY_TEMPLATE.format(**doc) + "\n\n"

    prompt = PROMT_TEMPLTE.format(question=query, context=context).strip()
    return prompt

def build_prompt_conversation(query, conversation, search_results):
    context = ""

    for doc in search_results:
        context = context + ENTRY_TEMPLATE.format(**doc) + "\n\n"

    prompt = PROMT_TEMPLTE_CONVERSATION.format(question=query, conversation=conversation, context=context).strip()
    return prompt


def llm(prompt, model="gpt-4o-mini"):
    response = client.chat.completions.create(
        model=model, messages=[{"role": "user", "content": prompt}]
    )

    answer = response.choices[0].message.content

    token_stats = {
        "prompt_tokens": response.usage.prompt_tokens,
        "completion_tokens": response.usage.completion_tokens,
        "total_tokens": response.usage.total_tokens,
    }

    return answer, token_stats


def rag(query, model="gpt-4o-mini"):
    t0 = time()

    search_results = elastic_search_hybrid(query)
    prompt = build_prompt(query, search_results)
    answer, token_stats = llm(prompt, model=model)

    relevance, rel_token_stats = evaluate_relevance(query, answer)

    t1 = time()
    took = t1 - t0

    openai_cost_rag = calculate_openai_cost(model, token_stats)
    openai_cost_eval = calculate_openai_cost(model, rel_token_stats)

    openai_cost = openai_cost_rag + openai_cost_eval

    answer_data = {
        "answer": answer,
        "model_used": model,
        "response_time": took,
        "relevance": relevance.get("Relevance", "UNKNOWN"),
        "relevance_explanation": relevance.get(
            "Explanation", "Failed to parse evaluation"
        ),
        "prompt_tokens": token_stats["prompt_tokens"],
        "completion_tokens": token_stats["completion_tokens"],
        "total_tokens": token_stats["total_tokens"],
        "eval_prompt_tokens": rel_token_stats["prompt_tokens"],
        "eval_completion_tokens": rel_token_stats["completion_tokens"],
        "eval_total_tokens": rel_token_stats["total_tokens"],
        "openai_cost": openai_cost,
    }

    return answer_data


def rag_conversation(query, conversation, model="gpt-4o-mini"):
    t0 = time()

    search_results = elastic_search_hybrid(query)
    prompt = build_prompt_conversation(query, conversation, search_results)
    answer, token_stats = llm(prompt, model=model)

    relevance, rel_token_stats = evaluate_relevance(query, answer)

    t1 = time()
    took = t1 - t0

    openai_cost_rag = calculate_openai_cost(model, token_stats)
    openai_cost_eval = calculate_openai_cost(model, rel_token_stats)

    openai_cost = openai_cost_rag + openai_cost_eval

    answer_data = {
        "answer": answer,
        "model_used": model,
        "response_time": took,
        "relevance": relevance.get("Relevance", "UNKNOWN"),
        "relevance_explanation": relevance.get(
            "Explanation", "Failed to parse evaluation"
        ),
        "prompt_tokens": token_stats["prompt_tokens"],
        "completion_tokens": token_stats["completion_tokens"],
        "total_tokens": token_stats["total_tokens"],
        "eval_prompt_tokens": rel_token_stats["prompt_tokens"],
        "eval_completion_tokens": rel_token_stats["completion_tokens"],
        "eval_total_tokens": rel_token_stats["total_tokens"],
        "openai_cost": openai_cost,
    }

    return answer_data
